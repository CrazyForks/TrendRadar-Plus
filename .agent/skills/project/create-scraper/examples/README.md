# æˆåŠŸçˆ¬è™«æ¡ˆä¾‹åº“

æœ¬ç›®å½•åŒ…å«ä»ç”Ÿäº§ç¯å¢ƒæ•°æ®åº“ä¸­æå–çš„**5ä¸ªæˆåŠŸè¿è¡Œçš„çˆ¬è™«è„šæœ¬**ï¼Œä½œä¸ºåˆ›å»ºæ–°çˆ¬è™«æ—¶çš„å‚è€ƒã€‚

## ğŸ“Š æ¡ˆä¾‹æ¦‚è§ˆ

| è„šæœ¬ | ç±»å‹ | æˆåŠŸæ¡ç›® | å…³é”®æŠ€æœ¯ |
|------|------|----------|----------|
| [wallstreetcn_flash.py](./wallstreetcn_flash.py) | JSON API | 167+ | å¤šé¢‘é“è½®è¯¢ã€å»é‡ã€æ’åº |
| [sina_tech_roll.py](./sina_tech_roll.py) | JSON API | 100+ | å¸¦å‚æ•°è¯·æ±‚ã€æ—¶é—´æˆ³è½¬æ¢ |
| [aibase_news.py](./aibase_news.py) | HTML è§£æ | 24+ | BeautifulSoup + Regex |
| [cls_depth_scraperapi.py](./cls_depth_scraperapi.py) | åŠ¨æ€æ¸²æŸ“ | 30+ | âš ï¸ ScraperAPI JS æ¸²æŸ“ |
| [nba_schedule_recursive.py](./nba_schedule_recursive.py) | åµŒå¥— JSON | 311+ | é€’å½’éå†ã€æ—¥æœŸè¿‡æ»¤ |

## ğŸ¯ ä½¿ç”¨æŒ‡å—

### 1. é€‰æ‹©åˆé€‚çš„æ¨¡æ¿

æ ¹æ®ç›®æ ‡ç½‘ç«™çš„æ•°æ®æºç±»å‹é€‰æ‹©ï¼š

```
æ•°æ®æ¥æºæ˜¯ JSON APIï¼Ÿ
â”œâ”€ ç®€å•å¹³é“ºç»“æ„ â†’ sina_tech_roll.py
â”œâ”€ å¤šä¸ªé¢‘é“/åˆ†ç±» â†’ wallstreetcn_flash.py
â””â”€ æ·±åº¦åµŒå¥—ç»“æ„ â†’ nba_schedule_recursive.py

æ•°æ®æ¥æºæ˜¯ HTML é¡µé¢ï¼Ÿ
â”œâ”€ é™æ€ HTML â†’ aibase_news.py
â””â”€ JavaScript åŠ¨æ€æ¸²æŸ“ â†’ cls_depth_scraperapi.py
```

### 2. å¤åˆ¶å¹¶ä¿®æ”¹

1. å¤åˆ¶æœ€æ¥è¿‘çš„æ¡ˆä¾‹è„šæœ¬
2. ä¿®æ”¹ URL å’Œè¯·æ±‚å‚æ•°
3. è°ƒæ•´æ•°æ®æå–é€»è¾‘ï¼ˆCSSé€‰æ‹©å™¨ã€JSON è·¯å¾„ç­‰ï¼‰
4. åœ¨ Admin åå°æµ‹è¯•è¿è¡Œ

### 3. æ³¨æ„å…³é”®å·®å¼‚

#### âš ï¸ ScraperAPIï¼ˆcls_depth_scraperapi.pyï¼‰

**ä½•æ—¶ä½¿ç”¨**ï¼šç›®æ ‡ç½‘ç«™ä½¿ç”¨ JavaScript åŠ¨æ€åŠ è½½å†…å®¹

**å‰ææ¡ä»¶**ï¼š
- åœ¨ Admin åå°å¼€å¯"ä½¿ç”¨ ScraperAPI"
- è®¾ç½®ç¯å¢ƒå˜é‡ `SCRAPERAPI_KEY`

**å…³é”®ä»£ç **ï¼š
```python
scraperapi_params = {"render": "true", "country_code": "us"}
resp = scraperapi_get(url, use_scraperapi, scraperapi_params=scraperapi_params, timeout=60)
```

#### ğŸ”„ é€’å½’ JSON è§£æï¼ˆnba_schedule_recursive.pyï¼‰

**ä½•æ—¶ä½¿ç”¨**ï¼šAPI è¿”å›æ·±åº¦åµŒå¥—çš„ JSONï¼Œä¸ç¡®å®šå…·ä½“å±‚çº§

**æ ¸å¿ƒæŠ€å·§**ï¼š
```python
stack = [data]
while stack:
    cur = stack.pop()
    if isinstance(cur, dict):
        if "ç›®æ ‡å­—æ®µ" in cur:  # è¯†åˆ«ç›®æ ‡å¯¹è±¡
            matches.append(cur)
        else:
            stack.extend(cur.values())  # ç»§ç»­é€’å½’
    elif isinstance(cur, list):
        stack.extend(cur)
```

## ğŸ“ DynamicPyProvider æ¥å£è§„èŒƒ

æ‰€æœ‰è„šæœ¬å¿…é¡»å®ç° `fetch(config, context)` å‡½æ•°ï¼š

```python
def fetch(config, context):
    # config: é…ç½®å‚æ•°ï¼ˆdictï¼‰
    # context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…å« use_scraperapi ç­‰ï¼‰
    
    # è¿”å›æ–°é—»åˆ—è¡¨
    return [
        {
            "title": "æ ‡é¢˜",
            "url": "é“¾æ¥",
            "time": "2026-01-15 10:00",  # å¯é€‰
            "rank": 1,                    # å¯é€‰
            "published_at": 1705284000    # å¯é€‰ï¼ŒUnixæ—¶é—´æˆ³
        }
    ]
```

## ğŸ› ï¸ æ²™ç®±ç¯å¢ƒå¯ç”¨æ¨¡å—

ä»¥ä¸‹æ¨¡å—å·²é¢„å¯¼å…¥ï¼Œå¯ç›´æ¥ä½¿ç”¨ï¼š
- `requests` - HTTP è¯·æ±‚
- `bs4` (BeautifulSoup) - HTML è§£æ
- `re` - æ­£åˆ™è¡¨è¾¾å¼
- `json` - JSON å¤„ç†
- `datetime` - æ—¶é—´å¤„ç†
- `time` - æ—¶é—´å·¥å…·
- `hashlib` - å“ˆå¸Œè®¡ç®—

## ğŸ’¡ å¸¸è§æŠ€å·§

### æ—¶é—´æ ¼å¼è½¬æ¢

```python
# Unix æ—¶é—´æˆ³ â†’ å­—ç¬¦ä¸²
import time
time_str = time.strftime("%Y-%m-%d %H:%M", time.localtime(timestamp))

# å­—ç¬¦ä¸² â†’ datetime
from datetime import datetime
dt = datetime.strptime("2026-01-15 10:00", "%Y-%m-%d %H:%M")
```

### å»é‡å¤„ç†

```python
seen = set()
for item in items:
    if item_id in seen:
        continue
    seen.add(item_id)
    # å¤„ç† item
```

### é”™è¯¯å®¹é”™

```python
try:
    # å¯èƒ½å‡ºé”™çš„ä»£ç 
    resp = requests.get(url, timeout=10)
except Exception as e:
    print(f"Error: {e}")
    return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯å´©æºƒ
```
