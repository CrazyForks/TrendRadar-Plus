"""
Mercury Chong Blogspot çˆ¬è™« (æç®€ç¨³å®šç‰ˆ)
ä¾èµ–: åç«¯å·²ä¿®å¤ HTTPS åè®®ï¼Œç›´æ¥è°ƒç”¨ scraperapi_get å³å¯
"""
def fetch(config, context):
    import requests
    from bs4 import BeautifulSoup
    
    url = "https://mercurychong.blogspot.com/"
    use_scraperapi = context.get("use_scraperapi", False)
    
    print(f"ğŸš€ å¼€å§‹æŠ“å–: {url}")
    
    try:
        if use_scraperapi:
            print("â³ é€šè¿‡ ScraperAPI (HTTPS) è¯·æ±‚...")
            # scraperapi_get æ˜¯åç«¯æ³¨å…¥çš„å…¨å±€å‡½æ•°
            # render=true ç¡®ä¿ JS æ¸²æŸ“ï¼Œcountry_code=us é¿å… GDPR å¼¹çª—
            resp = scraperapi_get(url, use_scraperapi=True, scraperapi_params={"render": "true", "country_code": "us"}, timeout=60)
        else:
            print("ğŸ”§ ç›´è¿æ¨¡å¼ (å¦‚å¤±è´¥è¯·å¼€å¯ ScraperAPI)...")
            resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
    except NameError:
        print("âš ï¸ æœ¬åœ°ç¯å¢ƒ: æœªæ‰¾åˆ° scraperapi_getï¼Œå°è¯•ç›´è¿...")
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
    except Exception as e:
        print(f"ğŸ’¥ è¯·æ±‚å¼‚å¸¸: {e}")
        return []

    if resp.status_code != 200:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {resp.status_code}")
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    items = []
    
    # Blogspot æ ‡å‡†ç»“æ„
    posts = soup.select(".post") 
    if not posts: posts = soup.select(".entry")
    
    print(f"âœ… æ‰¾åˆ° {len(posts)} ç¯‡æ–‡ç« ")
    
    for post in posts:
        title_tag = post.select_one(".post-title a, .entry-title a")
        if not title_tag: continue
        
        time_tag = post.select_one(".published")
        time_str = time_tag["title"] if (time_tag and time_tag.has_attr("title")) else ""
            
        items.append({
            "title": title_tag.get_text(strip=True),
            "url": title_tag.get("href"),
            "time": time_str
        })
        
    return items
